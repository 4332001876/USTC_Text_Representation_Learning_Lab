\documentclass{article}
%\documentclass[UTF8]{ctexart}
\usepackage{ctex}
% \usepackage{verbatim}  % comment env

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage[preprint]{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\title{文本表征学习Final: 基于预训练模型/大模型的情感分析}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  罗浩铭 \\
  PB21030838\\
  中国科学技术大学\ 安徽合肥 230026 \\
  \texttt{mzfslhm@mail.ustc.edu.cn} \\
  \And
  谢天 \\
  PB21061328\\
  中国科学技术大学\ 安徽合肥 230026 \\
  \texttt{xqs2002@mail.ustc.edu.cn} \\
  \And
  韦星光 \\
  PB21051056\\
  中国科学技术大学\ 安徽合肥 230026 \\
  \texttt{wesustc@mail.ustc.edu.cn} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
  本次实验完成了基于Transformer（tiny-BERT）模型的情感分类任务训练的全过程，包括数据处理、模型训练、性能评估，并对比分析了不同训练设定的性能。
  实验结果显示，模型性能随着hidden size的增大而略微提升，但提升并不明显。且模型即使是在充分预训练的情况下，性能仍然不如Doc2Vec模型。我们还测试了先进的Nomic Embedding模型，其性能远远超过了tiny-BERT模型以及Doc2Vec模型。
  Transformer架构是当今所有性能惊艳的大模型的基石。本次实验中对Transformer架构的探索，将为我们深入学习深度学习在NLP领域的应用提供更好的基础。
\end{abstract}


% 期末作业：基于预训练/大模型的情感分析
% 任务：使用自选模型实现情感分析，完成实验报告
% 数据集：情感分析数据集IMDB

% 需要的内容
% • 动机（包括方法选择、实验设定的原因等）
% • 实验细节（数据统计信息、模型参数等）
% • 实验结果分析（需要附主要实验结果的图表）
% • 讨论（实验中遇到的困难和解决方法等）
% • 组员分工及贡献情况说明
% • 参考文献列表


% 额外说明
% • 可根据实际算力情况仅使用其中部分训练数据（需在报告中写明）
% • 可以使用不同种类预训练模型，包括ChatGPT等大模型
% 加分项
% • 使用方法具有一定的创新性，具备进一步拓展为学术论文的潜力
% • 系统性比较多个预训练模型的效果
% • 使用额外的情感分析数据集比较分析结果
% • 系统全面地分析模型的效果，在主实验讨论分析之外，加入消融实验、错误分析、案例分析等

% • 提交：不超过4页（不含组员分工和参考文献）论文形式的报告
% • 包括：前言、方法（主要工作）、实验设定和结果、分析讨论、结论、组员分工、参考文献列表
% • 作业提交时间：6月23日00:00之前






\section{项目实现}
我们使用了Huggingface Datasets库及Huggingface Transformers库来实现我们的项目。
\subsection{数据处理}
对于MLM预训练数据集，我们将原数据集中的文本塞入\verb|LineByLineTextDataset|类中来构建自回归文本数据集（块大小最大为128），
然后使用\verb|DataCollatorForLanguageModeling|将其中15\%的token随机mask掉，以此来构建MLM预训练数据集。
% 对于MLM数据集，我们分别测试了加入无监督数据和不加入无监督数据的情况。

对于分类数据集，我们使用了Huggingface Datasets库来加载IMDB数据集，然后使用中的Tokenizer类来对文本进行分词。
分词过程中，我们限定每个文本的分词长度为512，对token长度超过512的文本进行，并对长度不足的文本填充\verb|[PAD]| token并加上相应的attention mask。


\subsection{模型实现}
由于BERT模型相当于是原生的Transformer模型的Encoder部分加上特殊的训练方式，因此我们的实验中主要使用了BERT式的模型来完成MLM预训练任务。

我们的模型以Huggingface中的\verb|prajjwal1/bert-tiny|模型为基础，\textbf{从头}对tiny-BERT模型进行训练。
对于模型架构，我们仅仅修改了hidden size为实验要求的100和200，并保持intermediate size是其四倍，其他参数不变。
我们的分词器直接借用了\verb|prajjwal1/bert-tiny|模型的分词器。

我们使用了Transformers库中的\verb|BertForMaskedLM|类来实现MLM预训练任务，使用上述自回归文本数据集来训练模型。其训练参数如下：
\begin{itemize}
  \item[$\bullet$] epoch数为10
  \item[$\bullet$] batch size为64
  \item[$\bullet$] 学习率为3e-4
  \item[$\bullet$] 使用AdamW优化器
  \item[$\bullet$] 使用cosine学习率调度器
  \item[$\bullet$] 使用warmup比例为0.1
  \item[$\bullet$] 使用weight decay为0.01
\end{itemize}
其余参数为Transformers库中\verb|TrainingArguments|类的默认参数。

最后，我们使用\verb|statsmodels|库中的逻辑回归类\verb|Logit|来完成文本情感分类任务，
使用训练好的tiny-BERT模型的\verb|[CLS]|位置对应输出来作为文本的向量表示。



\section{实验结果与分析}

我们使用的向量维度训得tiny-BERT模型，此后基于训得的tiny-BERT模型来训练文本情感分类的逻辑回归模型，将逻辑回归模型在测试集上的效果作为实验结果。
此外，我们还比较了预训练的\verb|prajjwal1/bert-tiny|模型以及当前先进的Embedding模型\verb|nomic-ai/nomic-embed-text-v1.5|模型（2024年2月份提出的近SOTA模型，基于BERT架构，参数量137M）在IMDB测试集上的性能。

得到的实验结果如表格\ref{tab:results}所示：
\begin{table}[htbp]
  \caption{\small{各种模型及参数设置下得到的Doc向量性能}}
  \label{tab:results}
  \centering
  \begin{tabular}{cccc}
    \toprule
    逻辑回归准确率          & Train   & Test    \\
    \midrule
    BERT (hidden size=100)  & $0.710$ & $0.697$ \\
    \midrule
    BERT (hidden size=200)  & $0.718$ & $0.709$ \\
    \midrule
    \verb|bert-tiny| & $0.766$ & $0.759$ \\
    \midrule
    \verb|nomic-embed-text-v1.5| & $0.945$ & $0.933$ \\
    \bottomrule
  \end{tabular}
\end{table}

从表格\ref{tab:results}中我们可以看出：
\begin{itemize}
  \item[$\bullet$] tiny-BERT模型在IMDB测试集上的性能随着hidden size的增大而有所提升，但提升并不明显。
  \item[$\bullet$] tiny-BERT模型的预训练效果不如Doc2Vec模型，这可能是因为tiny-BERT模型的归纳偏置较少，且训练数据量不足导致的。
  \item[$\bullet$] 在进行较为充分的预训练后，tiny-BERT模型的效果理论上可以提升约5\%，还是不如Doc2Vec模型，这说明Doc2Vec中对Paragraph vector进行迭代优化的过程可能是有很大用处的。
  \item[$\bullet$] 我们测试了先进的\verb|nomic-embed-text-v1.5|模型，其性能远远超过了tiny-BERT模型以及Doc2Vec模型。
\end{itemize}

% 3 epochs: Train accuracy:  0.64, Test accuracy:  0.62
% 10 epochs: Train accuracy:  0.67796, Test accuracy:  0.64824

% From now on, batch_size=256
% 5 epochs: Train accuracy:  0.698, Test accuracy:  0.65648

% pretrained mlm (hidden_size=128, batch_size=64, lr=3e-4): Train accuracy:  0.8094, Test accuracy:  0.78704

% MLM:
% 5 epochs: Train accuracy:  0.8094, Test accuracy:  0.78704
% 10 epochs, final params: Train accuracy:  0.7098, Test accuracy:  0.6968
% 10 epochs, final params, dim=200: Train accuracy:  0.718, Test accuracy:  0.70884
% 10 epochs, final params, dim=200, more-data: Train accuracy:  0.67632, Test accuracy:  0.66356

% pretrain: Train accuracy:  0.76628, Test accuracy:  0.759
% pretrain, average token: Train accuracy:  0.76948, Test accuracy:  0.75916

% nomic: Train accuracy:  0.945, Test accuracy:  0.93304

\end{document}

