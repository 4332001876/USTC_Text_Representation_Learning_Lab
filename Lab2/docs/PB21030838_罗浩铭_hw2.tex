\documentclass{article}
%\documentclass[UTF8]{ctexart}
\usepackage{ctex}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage[preprint]{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{文本表征学习HW2: Word2Vec模型}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  罗浩铭 \\
  PB21030838\\
  中国科学技术大学\ 安徽合肥 230026 \\
  \texttt{mzfslhm@mail.ustc.edu.cn} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph.
\end{abstract}

\section{项目实现}

% 简要描述处理数据和实现模型的过程，其中使用了哪些数据结构，遇到了哪些问题，是如何解决的
本项目主要基于谷歌的Word2Vec实现\url{https://code.google.com/archive/p/word2vec/source/default/source}来完成。

\subsection{数据处理}

我们使用enwiki8数据集作为训练数据。由于实验所用的代码仅靠空白字符（空格、换行等）来完成分词，
而enwiki8为XML格式套HTML格式，其中的内容混杂了大量与单词语义无关的格式符，因此我们需要对数据进行预处理。

我们去除了XML标签、HTML标签、HTML格式符、表格、图片等内容，只保留了文本内容。我们还在所有标点符号左右都添加了空格，并去除了其余所有特殊符号。
我们又将所有字母转为小写（但未做进一步的词干提取）。最终，我们得到了纯净的文本内容，用于后续实验。
（该处理主要借用了\texttt{demo-train-big-model-v1.sh}中的部分脚本代码来完成）


\subsection{模型实现}
我们的模型实现几乎全部基于谷歌的Word2Vec实现，仅仅编写了一些shell脚本来调用Word2Vec程序完成数据预处理、模型训练等任务。

模型评估部分我们编写了一个Python脚本来完成。我们首先先将Word2Vec训练得到的词向量读入程序，保存为一个从词语（改为全小写形式）到NumPy向量的字典映射。
然后对于测试集中的每对词语，我们使用余弦相似度来计算它们的相似度，并求其与label相似度的Spearman相关系数，作为实验结果。


\section{训练参数设置}
我们根据实验要求，设置参数为：词向量维度200，窗口大小5个词，词频过滤的阈值为5。

对于其它参数，我们进行如下设置：
采样阈值1e-4（使得词频高于该阈值的词语会被随机下采样），Negative Sampling数为25，学习率使用默认值（CBOW:0.05，SG:0.025），迭代轮数为15。



\section{实验结果与分析}
% 四种方法训练得到的词向量的性能结果

% 结合你对四种方法结果的观察，比较分析HS与NS，以及CBOW和SG

我们使用不同训练方法训得词向量，此后对于测试集中的每对词语（由于tableware不在词汇表中，我们略去了它所在的那对词语），
我们使用余弦相似度来计算它们的相似度，并求其与label中相似度的Spearman相关系数，作为实验结果。

得到的实验结果如表格\ref{tab:results}所示：
\begin{table}[htbp]
  \caption{Word2Vec模型在不同训练方法下得到的词向量性能}
  \label{tab:results}
  \vspace{5pt}
  \centering
  \begin{tabular}{cccc}
    \toprule
    Spearman相关系数（越高越好） & CBOW    & Skip-Gram \\
    \midrule
    Hierarchical Softmax         & $0.670$ & $0.721$   \\
    \midrule
    Negative Sampling            & $0.707$ & $0.672$   \\
    \bottomrule
  \end{tabular}
\end{table}

从表格\ref{tab:results}中我们可以按性能从高到低对四种方法排序得到：SG+HS > CBOW+NS > SG+NS > CBOW+HS。

Skip-Gram相较于CBOW在理论上性能更好，因为其训练组数更多，而Skip-Gram相较于CBOW在NS训练下性能更差可能是因为Negative Sampling时正负样本的比例差异导致。


在2核CPU的服务器上，Word2Vec模型迭代15轮次所需的训练时间如表格\ref{tab:time}所示：
\begin{table}[htbp]
  \caption{2核CPU的服务器上，Word2Vec模型在不同训练方法下迭代15轮次所需的训练时间}
  \label{tab:time}
  \vspace{5pt}
  \centering
  \begin{tabular}{cccc}
    \toprule
    训练用时             & CBOW   & Skip-Gram \\
    \midrule
    Hierarchical Softmax & 13m48s & 37m21s    \\
    \midrule
    Negative Sampling    & 26m26s & 131m33s   \\
    \bottomrule
  \end{tabular}
\end{table}

从表格\ref{tab:time}中我们可以看出，Skip-Gram相较于CBOW训练时长更长，Negative Sampling相较于Hierarchical Softmax训练时长更长。

\end{document}

